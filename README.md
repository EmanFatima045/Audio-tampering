# ğŸ§ Audio Forensics & Digital Tampering Detection

![Banner](./assets/banner.png)

## ğŸ“‘ Table of Contents

- ğŸ§  [Project Overview](#project-overview)
- ğŸ¯ [Core Objectives](#core-objectives)
- ğŸ› ï¸ [System Architecture](#system-architecture)
- ğŸ§ª [Training Workflow](#training-workflow)
- ğŸ“Š [Evaluation & Metrics](#evaluation--metrics)
- ğŸ”¥ [Bias Handling](#bias-handling)
- âš™ï¸ [Setup & Installation](#setup--installation)
- ğŸ™ [Acknowledgments](#acknowledgments)
- ğŸ’¼ [Libraries & Tools](#libraries--tools)
- ğŸ¤ [Contact & Contribution](#contact--contribution)
- ğŸ“œ [License](#license)

---

## ğŸ§  Project Overview

In the digital age, audio tampering is a critical threat to legal systems, journalism, and public trust. Our project provides a reliable solution to **detect forged audio**, **transcribe conversations**, and **analyze speaker emotions** to aid in digital forensics and investigation.

This system:
- Detects **audio tampering**
- Performs **speech transcription** using **Whisper**
- Analyzes **voice emotion** as positive, negative, or neutral

---

## ğŸ¯ Core Objectives

Our main goals are:

- âœ… Detect and classify tampered vs untampered audio
- ğŸ™ï¸ Transcribe spoken content using OpenAI's **Whisper**
- ğŸ’¬ Perform **audio sentiment analysis** to classify emotion
- ğŸ§ª Ensure scalability, performance, and forensic-grade reliability
- ğŸ” Empower cyber and digital forensic professionals with intelligent tools

---

## ğŸ› ï¸ System Architecture

Our system architecture involves an end-to-end pipeline built with modern tools and clear separation of responsibilities:

```text
ğŸ“¤ Upload Audio File (React.js Frontend)
â†“
ğŸ”Š Preprocess Audio (pydub, librosa, scipy)
â†“
ğŸ§  Tampering Detection (Random Forest Classifier)
â†“
ğŸ—£ Transcription (Whisper ASR)
â†“
ğŸ˜Š Sentiment Analysis (SVM/ML Classifier)
â†“
ğŸ“Š JSON Response Displayed on Frontend
## ğŸ§ª Training Workflow

### 1. Dataset Collection
We collected audio samples from sources like **ASVspoof**, **Kaggle**, and our own recordings. Two primary datasets were used:
- **Tampered vs Original** for forgery detection
- **Positive, Negative, Neutral** for sentiment analysis

### 2. Feature Extraction
We used **MFCC (Mel-Frequency Cepstral Coefficients)** via `librosa` to extract meaningful acoustic patterns.

```python
mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
features = np.mean(mfcc.T, axis=0)
